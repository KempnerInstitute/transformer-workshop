{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJjyDCYiycRG"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yy3ncD8pycRH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDLagMhycRH"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rgppY_zycRH"
      },
      "source": [
        "## Exercise: Implementing Character-based tokenization\n",
        "\n",
        "1. Get a sorted list of every unique character in your training data.\n",
        "2. Create a dictionary that converts tokens to IDs (str_to_int) and one that converts IDs to tokens (int_to_str)\n",
        "3. Implement functions encode and decode.\n",
        "Encode should take in a string and output list of token IDs.\n",
        "Decode should take in a list of token IDs and output a string.\n",
        "4. Test encoding and then decoding “My dog Leo is extremely cute.” Do you recover the correct string?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JnmqJ-922AD_"
      },
      "outputs": [],
      "source": [
        "# Load in all training data\n",
        "with open('tiny_wikipedia.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl73KMyZycRH",
        "outputId": "b7d9d16c-a7f6-4573-ceb6-1aec4a09cec0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
        "# Hint: set is useful for getting unique elements in a sequence\n",
        "... # your code here\n",
        "\n",
        "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
        "... # your code here\n",
        "\n",
        "# Step 3) Define encode and decode functions\n",
        "# def encode(...):\n",
        "#     ...\n",
        "\n",
        "# def decode(...):\n",
        "#     ...\n",
        "\n",
        "# Step 4) Test your implementation on \"My dog Leo is extremely cute.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRkLNO4B4hKq"
      },
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pCQAN_QY3zPO"
      },
      "outputs": [],
      "source": [
        "# Step 1) Get a sorted list of all unique characters that occur in this text\n",
        "# Hint: set is useful for getting unique elements in a sequence\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# Step 2) Create the dictionaries str_to_int and int_to_str\n",
        "str_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_str = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Step 3) Define encode and decode functions\n",
        "def encode(text, str_to_int):\n",
        "    ids = [str_to_int[c] for c in text]\n",
        "    return ids\n",
        "\n",
        "def decode(ids, int_to_str):\n",
        "    text_list = [int_to_str[id] for id in ids]\n",
        "    return ''.join(text_list)\n",
        "\n",
        "# Step 4) Test your implementation on \"My dog Leo is extremely cute.\"\n",
        "input_text = \"My dog Leo is extremely cute.\"\n",
        "ids = encode(input_text, str_to_int)\n",
        "decoded_text = decode(ids, int_to_str)\n",
        "assert input_text == decoded_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKsH64ZycRI"
      },
      "source": [
        "## Tokenize input data and create splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rVKYkgLuycRI"
      },
      "outputs": [],
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text, str_to_int), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split, ctx_len, batch_size, device='cpu'):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - ctx_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ctx_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ctx_len+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define our transformer parameters with a config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 256 # the model/hidden/embedding dim\n",
        "    n_heads: int = 4 # number of attention heads (width)\n",
        "    ctx_len: int = 64 # context length\n",
        "    batch_size: int = 8 # batch size\n",
        "    n_layers: int = 12 # number of layers (depth)\n",
        "    vocab_size: int = -1 # vocab size, to be determined once we have created a tokenizer\n",
        "\n",
        "    def set_vocab_size(self, vocab_size):\n",
        "        self.vocab_size = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config()\n",
        "config.set_vocab_size(vocab_size=len(chars)) # set our vocabular size (equal to the number of chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbNpCKoSycRI"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Implementing single headed causal self attention\n",
        "\n",
        "Self-attention is a core mechanism in transformers that allows each position in a sequence to attend to all previous positions. The \"causal\" part ensures each position can only attend to past positions - this is crucial for language modeling.\n",
        "\n",
        "The task is to fill out the `SingleHeadCausalAttention` module.  The `forward(self, x)` function that will take in an input `x` that is `(B, T, C)`-dimensional corresponding to batch size, sequence length, and hidden dimension and then output the result after applying the attention formula.  Note that the causal mask has been already defined for you (it is a lower triangular matrix whose entries are 1's.  You can refer to it by calling `self.cmask`.)\n",
        "\n",
        "1. Create the K, Q, V matrices that are the resultant matrices after applying the `self.key`, `self.query`, and `self.values` projections.\n",
        "2. Compute and return attention using the formula:\n",
        "\n",
        "$$\\textrm{attention}(K, V, Q) = \\textrm{softmax}\\left( c \\odot \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V $$\n",
        "\n",
        "where $c \\odot \\dots$ denotes the application of the causal mask.  You can use `torch.masked_fill(...)` here to apply the mask.  It takes as input three arguments: the input matrix you want to mask, where you want to mask it (a boolean condition), and the value you want to mask with.  To figure out what value you want to mask with, it may be helpful to recall the softmax formula; the $i$-th component of a vector $u$ after a softmax is: $$ \\textrm{softmax}(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}.$$\n",
        "\n",
        "Hints:\n",
        "1. Keep track of the matrix dimensions after each step!\n",
        "2. Note that you can transpose a matrix in Pytorch by calling `A.transpose(dim_1, dim_2)` where `dim_1`, `dim_2` refer to the dimensions you want to transpose.\n",
        "3. You may use Pytorch's built-in softmax function `F.softmax(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadCausalAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Calculate the dimension for each attention head\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "        \n",
        "        # TODO: Initialize the Key, Query, and Value projections\n",
        "        # Each should be a linear layer that projects from d_model to head_dim\n",
        "        # Hint: Use nn.Linear(..., bias=False) as is standard in attention\n",
        "        self.key = ... # Your code here\n",
        "        self.query = ... # Your code here\n",
        "        self.values = ... # Your code here\n",
        "        \n",
        "        # Create causal mask (lower triangular matrix), you an refer to it by `self.cmask`\n",
        "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        # TODO Step 1: Project input to get Key, Query, Value matrices\n",
        "        K = ... # Your code here\n",
        "        Q = ... # Your code here\n",
        "        V = ... # Your code here\n",
        "        \n",
        "        # TODO Step 2: Compute attention scores and apply mask\n",
        "        # Remember: \n",
        "        # - Scale by sqrt(head_dim)\n",
        "        # - Use the causal mask (self.cmask) to prevent attention to future tokens\n",
        "        # - Apply softmax to get attention weights\n",
        "        # - Multiply with values\n",
        "        \n",
        "        # Your implementation here...\n",
        "        \n",
        "        return # Final output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
        "attention = SingleHeadCausalAttention(config)\n",
        "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
        "output = attention(x)\n",
        "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadCausalAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "        self.key = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "        self.query = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "        self.values = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
        "\n",
        "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        K = self.key(x) # (B, T, C) @ (_, C, H) -> (B, T, H)\n",
        "        Q = self.query(x)\n",
        "        V = self.values(x)\n",
        "\n",
        "        y = Q @ K.transpose(-2, -1) * self.head_dim**-0.5 # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
        "        y = torch.masked_fill(y, self.cmask[:T, :T]==0, float('-inf'))\n",
        "        y = F.softmax(y, dim=-1) @ V\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-head self attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: implementing multi-head attention\n",
        "\n",
        "The task is to write the multi-headed self attention module.  You should not need to write more than a few lines of code here.\n",
        "\n",
        "1. Define `self.heads` as the list of attention heads that will act in parallel on the input.  You may use `nn.ModuleList(...)` to do this.\n",
        "2. Define `self.linear`, a linear projection.\n",
        "3. Define the forward function which will take in the input `x` (which is (B, T, C)-dimesional), pass it through each head, and concatenate the output.  To perform concatenation you can use `torch.cat(...)`.\n",
        "4. After going through the attention heads, the input should then go through the linear projection and then returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadCausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = ... # your code here\n",
        "        self.linear = ... # your code here\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        ... # your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
        "mha = MultiHeadCausalAttention(config)\n",
        "\n",
        "# Test with small batch\n",
        "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
        "out = mha(x)\n",
        "assert out.shape == (2, 10, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadCausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SingleHeadCausalAttention(config) for _ in range(config.n_heads)])\n",
        "        self.linear = nn.Linear(config.d_model, config.d_model)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        y = self.linear(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the feed-forward network (FFN) decoder block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: FFN\n",
        "\n",
        "The Feed-Forward Network (FFN) is a simple yet powerful component that applies two linear transformations with a ReLU activation in between. The first transformation expands the input dimension by a factor of 4, and the second transformation projects it back to the original dimension.  In this exercise, you will implement this module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # TODO: Initialize two linear layers\n",
        "        # First layer should expand from d_model to 4*d_model\n",
        "        # Second layer should project back to d_model\n",
        "        # Hint: use nn.Linear(in_features, out_features)\n",
        "        self.l1 = # Your code here\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = # Your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "        # 1. Apply first linear layer\n",
        "        # 2. Apply ReLU activation\n",
        "        # 3. Apply second linear layer\n",
        "        x = ... # Your code here\n",
        "        x = ... # Your code here\n",
        "        x = ... # Your code here\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Decoder Block\n",
        "\n",
        "The Decoder Block is a core component that combines self-attention with a feed-forward network. It uses residual connections and layer normalization to help with training stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadCausualAttention(config)\n",
        "        # TODO: Initialize layer normalization layers\n",
        "        # Hint: use nn.LayerNorm(config.d_model)\n",
        "        self.ln1 = # Your code here\n",
        "        self.ffn = FFN(config)\n",
        "        self.ln2 = # Your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass with residual connections\n",
        "        # Remember the pattern: x = x + sublayer(layer_norm(x))\n",
        "        x = ... # Your code here  # First attention block with residual\n",
        "        x = ... # Your code here  # Second FFN block with residual\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = Config(d_model=256)\n",
        "ffn = FFN(config)\n",
        "decoder = DecoderBlock(config)\n",
        "\n",
        "# Test with random input\n",
        "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
        "output = decoder(x)\n",
        "assert output.shape == x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(config.d_model, 4*config.d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(4*config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "    \n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadCausalAttention(config)\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ffn = FFN(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffn(self.ffn(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the transformer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're now ready to put the components together into our final decoder module that can actually generate text! Your task to implement the missing pieces of the Decoder class. This is the top-level module that:\n",
        "\n",
        "* Embeds input tokens and adds positional information\n",
        "* Processes them through multiple transformer layers\n",
        "* Outputs predictions for the next token through the `forward(...)` function\n",
        "* Can generate new sequences autoregressively through the `generate(...)` function\n",
        "\n",
        "We have given extra hints for this module since it is a challenging exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Stack of decoder blocks\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "\n",
        "        # TODO: Initialize components\n",
        "        # Final layer norm and projection to vocabulary\n",
        "        self.ln = ... # normalize to d_model dimension\n",
        "        self.lin = ... # project from d_model to vocab_size\n",
        "        \n",
        "        # Embeddings\n",
        "        self.wte = ... # Your code here, token embedding: vocab_size → d_model\n",
        "        self.wpe = ... # Your code here, position embedding: ctx_len → d_model\n",
        "        \n",
        "        # Loss function for training\n",
        "        self.L = nn.CrossEntropyLoss()\n",
        "        self.ctx_len = config.ctx_len\n",
        "    \n",
        "    def forward(self, x, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tokens (B, T)\n",
        "            targets: Optional target tokens (B, T)\n",
        "        Returns:\n",
        "            logits: Predictions (B, T, vocab_size)\n",
        "            loss: Optional cross-entropy loss\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        \n",
        "        # TODO Step 1: Get embeddings\n",
        "        # Convert tokens to embeddings and add positional information\n",
        "        x_tok = self.wte(x)         # (B, T, d_model)\n",
        "        x_pos = self.wpe(torch.arange(T))        # (B, T, d_model)\n",
        "        x = ... # Your code here        # Add the embeddings together\n",
        "        \n",
        "        # TODO Step 2: Process through transformer\n",
        "        x = self.blocks(x)          # Apply transformer blocks\n",
        "        x = ... # Your code here        # Apply final layer norm\n",
        "        logits = ... # Your code here   # Project to vocabulary size\n",
        "        \n",
        "        # TODO Step 3: Compute loss if targets are provided\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape logits and targets for loss computation\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)    # Combine batch and time dimensions\n",
        "            targets = targets.view(B*T)      # Flatten targets\n",
        "            loss = ... # Your code here          # Compute cross entropy loss\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_len=256):\n",
        "        \"\"\"\n",
        "        Generate new tokens given initial sequence idx.\n",
        "        \"\"\"\n",
        "        # TODO: Implement generation loop\n",
        "        for _ in range(max_len):\n",
        "            # Step 1: Take the last ctx_len tokens\n",
        "            idx_window = ... # Your code here\n",
        "            \n",
        "            # Step 2: Get model predictions\n",
        "            logits, _ = self(idx_window)     # (B, T, V)\n",
        "            logits = logits[:, -1, :]        # Only take the last token's predictions\n",
        "            \n",
        "            # Step 3: Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            \n",
        "            # Step 4: Append to sequence\n",
        "            idx = ... # Your code here\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8])\n"
          ]
        }
      ],
      "source": [
        "config = Config(\n",
        "    vocab_size=100,\n",
        "    d_model=256,\n",
        "    ctx_len=64,\n",
        "    n_layers=4\n",
        ")\n",
        "decoder = Decoder(config)\n",
        "\n",
        "x = torch.randint(0, 100, (1, 10))\n",
        "logits, loss = decoder(x, x)\n",
        "\n",
        "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
        "print(out.shape)  # Should be (1, 8) - original 3 tokens + 5 new ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
        "        self.ln = nn.LayerNorm(config.d_model)\n",
        "        self.lin = nn.Linear(config.d_model, config.vocab_size)\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.wpe = nn.Embedding(config.ctx_len, config.d_model)\n",
        "        self.L = nn.CrossEntropyLoss()\n",
        "        self.ctx_len = config.ctx_len\n",
        "    \n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "        x_tok = self.wte(x)\n",
        "        x_pos = self.wpe(torch.arange(T))\n",
        "        x = x_tok + x_pos # (B, T, C)\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lin(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # compute xentropy loss, targets are (B, T)\n",
        "            B, T, V = logits.shape\n",
        "            targets = targets.view(B*T)\n",
        "            logits = logits.view(B*T, V)\n",
        "            loss = self.L(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_len=256):\n",
        "        for _ in range(max_len):\n",
        "            idx_window = idx[:, -self.ctx_len:]\n",
        "            logits, _ = self(idx_window) #(B, T, V)\n",
        "            logits = logits[:,-1,:]\n",
        "            prob = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(prob, num_samples=1) # greedy sample\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
