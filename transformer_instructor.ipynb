{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbd4a4a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KempnerInstitute/transformer-workshop/blob/main/transformer_instructor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJjyDCYiycRG"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy3ncD8pycRH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbDLagMhycRH"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "First, we'll load the training data (`tiny_wikipedia.txt`), which you can find [here](https://github.com/KempnerInstitute/transformer-workshop/blob/main/tiny_wikipedia.txt), and set up tokenization. To do this, we identify all unique characters in the dataset and assign each one a unique integer ID.\n",
    "\n",
    "We then define two functions:\n",
    "\n",
    "`encode(text)`: converts text into integer token IDs\n",
    "\n",
    "`decode(token_ids)`: converts integer token IDs back into text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnmqJ-922AD_"
   },
   "outputs": [],
   "source": [
    "# Load in all training data\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KempnerInstitute/transformer-workshop/main/tiny_wikipedia.txt\"\n",
    "response = requests.get(url)\n",
    "with open(\"tiny_wikipedia.txt\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "with open('tiny_wikipedia.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCQAN_QY3zPO",
    "outputId": "a288f101-2ee8-43f1-9800-9ba4953e3c29"
   },
   "outputs": [],
   "source": [
    "# Get all unique characters in the training data\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Create mappings from characters to integers and back\n",
    "str_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_str = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define encode (text → ids) and decode (ids → text) functions\n",
    "def encode(text, str_to_int):\n",
    "  \"\"\"Convert a string into a list of integer token IDs.\"\"\"\n",
    "  ids = [str_to_int[c] for c in text]\n",
    "  return ids\n",
    "\n",
    "def decode(ids, int_to_str):\n",
    "  \"\"\"Convert a list of integer token IDs back into a string.\"\"\"\n",
    "  text_list = [int_to_str[id] for id in ids]\n",
    "  return ''.join(text_list)\n",
    "\n",
    "# Test the implementation\n",
    "input_text = \"My dog Leo is extremely cute.\"\n",
    "ids = encode(input_text, str_to_int)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Token IDs: {ids}\")\n",
    "\n",
    "decoded_text = decode(ids, int_to_str)\n",
    "assert input_text == decoded_text, \"Decoded text does not match input\"\n",
    "print(\"Decoded text matches the original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUKsH64ZycRI"
   },
   "source": [
    "## Tokenize input data and create splits\n",
    "\n",
    "Next, we’ll convert our tokenized text into a PyTorch tensor and split it into training and validation sets. The training data will be used to learn model parameters, while the validation set lets us check how well the model generalizes to unseen text.\n",
    "\n",
    "We'll also define a helper function, `get_batch`, which randomly samples small chunks of text from the dataset. Each row in the returned batch corresponds to a different training example — a sequence of `ctx_len` (context length) consecutive tokens. The input tensor `x` contains these sequences, and the target tensor `y` contains the same sequences shifted by one position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sQeISTsneo1U"
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to get helper function get_batch to generate batches of data\n",
    "def get_batch(split, ctx_len, batch_size, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a small batch of input (x) and target (y) sequences.\n",
    "\n",
    "    Args:\n",
    "        split (str): 'train' or 'val'\n",
    "        ctx_len (int): length of each sequence (context window)\n",
    "        batch_size (int): number of sequences per batch\n",
    "        device (str): device to move the tensors to ('cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "        x, y (torch.Tensor): tensors of shape (batch_size, ctx_len)\n",
    "    \"\"\"\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - ctx_len, (batch_size,))\n",
    "\n",
    "    # Each x is a sequence of ctx_len tokens; y is the same sequence shifted by one\n",
    "    x = torch.stack([data_split[i:i+ctx_len] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+ctx_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVKYkgLuycRI"
   },
   "outputs": [],
   "source": [
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text, str_to_int), dtype=torch.long)\n",
    "n = int(0.9 * len(data)) # first 90% will be train, remaining 10% for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYHkPWYeXMVE"
   },
   "source": [
    "We’ll take a look at a small batch to see how the input (x) and target (y) sequences align (by decoding them back to text). Each row represents one data point — a short sequence of text that the model uses for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxIgP3aZWIzn",
    "outputId": "1936f321-7876-42a2-bd69-e0aa60341816"
   },
   "outputs": [],
   "source": [
    "# Let's grab a batch to look at the dimensions\n",
    "\n",
    "x, y = get_batch(split='train', ctx_len=64, batch_size=8)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Show a few examples to see how x and y align\n",
    "for i in range(3):\n",
    "    print(f\"\\nData point {i}\")\n",
    "    print(\"x:\", decode(x[i].tolist(), int_to_str))\n",
    "    print(\"y:\", decode(y[i].tolist(), int_to_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7Ai3T12KCmZ"
   },
   "source": [
    "# Model configuration\n",
    "\n",
    "We’ll store all the model’s key hyperparameters in a configuration class.\n",
    "This makes it easy to keep track of settings (like model size, number of layers, and context length) and to modify them later without changing code in multiple places.\n",
    "\n",
    "The Config class below defines these values and includes a helper method to update the vocabulary size once we’ve built the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3skDcLieKCmZ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "  \"\"\"Configuration settings for the Transformer model.\"\"\"\n",
    "  d_model: int = 256 # hidden dimension (embedding size)\n",
    "  n_heads: int = 4 # number of attention heads (width)\n",
    "  ctx_len: int = 64 # context length\n",
    "  batch_size: int = 8 # batch size\n",
    "  n_layers: int = 12 # number of layers (depth)\n",
    "  vocab_size: int = -1 # vocab size, to be determined once we have created a tokenizer\n",
    "  device: str = 'cpu'\n",
    "\n",
    "  def set_vocab_size(self, vocab_size):\n",
    "    \"\"\"Update the vocabulary size once tokenization is defined.\"\"\"\n",
    "    self.vocab_size = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF7uhbkFYAZO"
   },
   "outputs": [],
   "source": [
    "# Initialize the configuration and set the vocabulary size\n",
    "config = Config()\n",
    "config.set_vocab_size(vocab_size=len(chars))  # number of unique characters in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eucjTISYKCmZ"
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGENv-a0KCmZ"
   },
   "source": [
    "## Exercise: Implement token embeddings\n",
    "\n",
    "We’ll now implement a class that converts token IDs into their corresponding embedding vectors.\n",
    "Given a batch of token IDs of shape (batch size by context length), the embedding layer should output token embeddings of shape batch size by (context length by embedding dimension)\n",
    "\n",
    "Use nn.Embedding (docs [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)) to map each integer token ID to a learnable embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42h2kDx5bcZw"
   },
   "source": [
    "```python\n",
    "class TokenEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Create the token embedding layer by specifying arguments to nn.Embedding\n",
    "        self.wte = nn.Embedding(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # TODO: Get forward pass of token embedding layer\n",
    "        x_tok = ...\n",
    "\n",
    "        return x_tok\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "token_embedding = TokenEmbeddingLayer(config)\n",
    "x_tok = token_embedding(xb)\n",
    "\n",
    "assert x_tok.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\"\n",
    "print(\"Token embedding layer output shape is correct!\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQo-eU3_KCmZ",
    "outputId": "116c47f1-e3f2-4e17-fe2c-dff7d7db0c95"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class TokenEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        x_tok = self.wte(x)\n",
    "\n",
    "        return x_tok\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "token_embedding = TokenEmbeddingLayer(config)\n",
    "x_tok = token_embedding(xb)\n",
    "\n",
    "assert x_tok.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\"\n",
    "print(\"Token embedding layer output shape is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_4AfJRrYZJJ"
   },
   "source": [
    "**Reflect & discuss**:\n",
    "\n",
    "Take a moment to think about what your embedding layer is doing before we move on:\n",
    "\n",
    "*   What does each row of the embedding matrix represent? How does the output shape of the embedding layer relate to the input shape?\n",
    "*  How does the embedding layer learn during training?\n",
    "* What kind of information do the embedding vectors capture as the model trains?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzPkz5MWm9Sf"
   },
   "source": [
    "Hint **Reflect & discuss**:\n",
    "\n",
    "Take a moment to think about what your embedding layer is doing before we move on:\n",
    "\n",
    "*   What does each row of the embedding matrix represent? How does the output shape of the embedding layer relate to the input shape?\n",
    "\n",
    "*Each row of the embedding matrix (self.wte.weight) corresponds to a token in the vocabulary.\n",
    "That row is a learned vector representation of that token, which the model will adjust during training so that tokens used in similar contexts end up with similar vectors. The embedding matrix has shape (vocab_size, d_model).\n",
    "When we feed in a batch of token IDs shaped (batch_size, seq_len), the embedding layer looks up the appropriate rows for each token, producing an output tensor of shape (batch_size, seq_len, d_model).*\n",
    "\n",
    "*  How does the embedding layer learn during training?\n",
    "\n",
    "*The embedding weights are trainable parameters.\n",
    "During the forward pass, token IDs are used to look up embeddings.\n",
    "During backpropagation, gradients flow through those lookups and update the corresponding rows in the embedding matrix.*\n",
    "\n",
    "* What kind of information do the embedding vectors capture as the model trains?\n",
    "\n",
    "*As the model learns, embeddings start to capture statistical and semantic relationships between tokens — tokens that appear in similar contexts (e.g., “dog” and “cat”) move closer together in the embedding space.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eslUrQkvKCmZ"
   },
   "source": [
    "## Advanced Exercise: Implement full embedding layer\n",
    "\n",
    "Now we'll combine token and position embeddings into a single embedding layer. You can reuse your token embedding code from before. Think about how to represent position embeddings so the model knows *where* each token appears in the sequence. This is a little tricky so feel free to click on the hints below the exercise for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnNuJWc9bjfz"
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = config.device\n",
    "\n",
    "        # TODO: Create the token embedding layer (same as before)\n",
    "        self.wte = nn.Embedding(...)\n",
    "\n",
    "        # TODO: implement position embedding\n",
    "        self.wpe = ...\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # TODO: Compute token and position embeddings\n",
    "        x_tok = ...\n",
    "        x_pos = ...\n",
    "\n",
    "        # Combine token and position embeddings\n",
    "        x_embeddings = x_tok + x_pos\n",
    "\n",
    "        return x_embeddings\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "embedding = EmbeddingLayer(config)\n",
    "x_embedding = embedding(xb)\n",
    "\n",
    "assert x_embedding.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\"\n",
    "print(\"Embedding layer output shape is correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "K7FwD6GKeQgq"
   },
   "outputs": [],
   "source": [
    "# @markdown **Click to see hint #1**\n",
    "\"\"\"\n",
    "For position embeddings, you can also use nn.Embedding.\n",
    "Instead of the first dimension being equal to vocab size,\n",
    "it should be equal to the context length (so you learn an\n",
    "embedding for each position in a sequence)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab_type": "text",
    "id": "CZgfUBpzeeIJ"
   },
   "source": [
    "```python\n",
    "# @markdown **Click to see hint #2**\n",
    "\n",
    "\"\"\"\n",
    "The output of the token embeddings forward pass has shape\n",
    "(batch size x context length x model dimension).\n",
    "\n",
    "For the forward pass of the position embeddings, you only\n",
    "need to create a matrix of shape (context length by model\n",
    "dimension) because nothing depends on the actual data in\n",
    "each batch. Broadcasting will ensure you can still add\n",
    "this matrix to the token embeddings.\n",
    "\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNR0iuwyKCma",
    "outputId": "a816cae4-06df-47a0-9125-d9c961a0575d"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.wpe = nn.Embedding(config.ctx_len, config.d_model)\n",
    "        self.device = config.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        x_tok = self.wte(x)\n",
    "        # print(x_tok.shape) uncomment this if you want to see the shape of above tensor\n",
    "        x_pos = self.wpe(torch.arange(seq_len, device=self.device))\n",
    "        # print(x_pos.shape)\n",
    "\n",
    "        # Combine token and position embeddings\n",
    "        x_embeddings = x_tok + x_pos\n",
    "\n",
    "        return x_embeddings\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "embedding = EmbeddingLayer(config)\n",
    "x_embedding = embedding(xb)\n",
    "\n",
    "assert x_embedding.shape == (config.batch_size, config.ctx_len, config.d_model), \"Embedding dimensions are incorrect\"\n",
    "print(\"Embedding layer output shape is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkhS81WNqdWO"
   },
   "source": [
    "**Reflect and Discuss**\n",
    "\n",
    "\n",
    "*   Why does the model need position embeddings in addition to token embeddings?\n",
    "\n",
    "*   Why are we adding the token and position embeddings together, instead of concatenating them?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAXWmbUWq6VW"
   },
   "source": [
    "Hint **Reflect and Discuss**\n",
    "\n",
    "\n",
    "*   Why does the model need position embeddings in addition to token embeddings?\n",
    "\n",
    "*Token embeddings tell the model what each token is, but not where it appears in the sequence. Without position information, the model would treat text as a “bag of words,” unable to distinguish between different word orders (e.g., “dog bites man” vs. “man bites dog”). So, position embeddings give the model a sense of order and structure, which is essential for understanding sequences.*\n",
    "\n",
    "*   Why are we adding the token and position embeddings together, instead of concatenating them?\n",
    "\n",
    "*Both token and position embeddings have the same dimensionality (d_model), meaning each represents information in the same feature space. By adding them, we combine what the token is (its identity) and where it is (its position) into a single vector of the same size. This keeps the total embedding dimension fixed — so the next layers of the model (attention, feedforward, etc.) can process the combined information without any change in shape.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbNpCKoSycRI"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhDF833kKCma"
   },
   "source": [
    "## Exercise: Implementing single headed causal self attention\n",
    "\n",
    "Self-attention is a core mechanism in transformers that allows each position in a sequence to attend to all previous positions. The \"causal\" part ensures each position can only attend to past positions - this is crucial for language modeling.\n",
    "\n",
    "In this exercise, you'll fill out the `SingleHeadCausalAttention` module below.  \n",
    "\n",
    "The `__init__` method should define the key, query, and value projection layers.  A causal mask (`self.cmask`) is already provided for you (it’s a lower-triangular matrix of 1s that enforces the causal constrain).\n",
    "\n",
    "The `forward(self, x)` should implement the full attention computation:\n",
    "$$\\textrm{attention}(K, V, Q) = \\textrm{softmax}\\left( c \\odot \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V $$\n",
    "where $c \\odot \\dots$ denotes the application of the causal mask.  To do this,\n",
    "\n",
    "1. Project the input `x` into the K, Q, V matrices using the `self.key`, `self.query`, and `self.values` projections.\n",
    "2. Compute scaled attention scores\n",
    "\n",
    "$$\\frac{Q K^\\top}{\\sqrt{d_k}} $$\n",
    "\n",
    "3. Apply the causal mask. You can use `torch.masked_fill(...)` to apply the mask.  This function takes three arguments: the input matrix you want to mask, where you want to mask it (a boolean condition), and the value you want to mask with.  Think about what value you should fill masked positions with so that after the softmax they have (essentially) zero probability. It may be helpful to recall the softmax formula; the $i$-th component of a vector $u$ after a softmax is: $$ \\textrm{softmax}(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}.$$\n",
    "\n",
    "4. Apply the softmax and compute the weighted sum with V.\n",
    "\n",
    "\n",
    "Hints:\n",
    "1. Keep track of the tensor dimensions after each step!\n",
    "2. You can transpose tensors in Pytorch by calling `A.transpose(dim_1, dim_2)` where `dim_1`, `dim_2` refer to the dimensions you want to transpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VD5irEDxboX2"
   },
   "source": [
    "```python\n",
    "class SingleHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the dimension for each attention head\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "\n",
    "        # TODO: Initialize the Key, Query, and Value projections\n",
    "        # Each projects from d_model to head_dim with no bias\n",
    "        self.key = nn.Linear(..., ...,  bias=False)\n",
    "        self.query = nn.Linear(..., ...,  bias=False)\n",
    "        self.value = nn.Linear(..., ...,  bias=False)\n",
    "\n",
    "        # Create causal mask (lower triangular matrix), you an refer to it by `self.cmask`\n",
    "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # TODO Step 1: Compute K, Q, V projections\n",
    "        K = ...\n",
    "        Q = ...\n",
    "        V = ...\n",
    "\n",
    "        # TODO Step 2: Compute attention scores\n",
    "        attention_scores = ...\n",
    "\n",
    "        # TODO Step 3: Apply the causal mask (you can use `torch.masked_fill(...)` here)\n",
    "        ...\n",
    "\n",
    "        # TODO Step 4: Applyl the softmax and compute the weighted sum with V\n",
    "        ...\n",
    "\n",
    "        return # Final output\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "attention = SingleHeadCausalAttention(config)\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
    "output = attention(x)\n",
    "\n",
    "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32\n",
    "print(\"Single-head causal attention output shape is correct!\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dx1AwVbsKCma",
    "outputId": "99852599-27b7-44bd-997d-aa3212b24535"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class SingleHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.key = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "        self.query = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, self.head_dim, bias=False)\n",
    "\n",
    "        self.register_buffer(\"cmask\", torch.tril(torch.ones([config.ctx_len, config.ctx_len])))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Step 1: Compute K, Q, V projections\n",
    "        K = self.key(x) # (batch_size, seq_len, head_dim)\n",
    "        Q = self.query(x) # (batch_size, seq_len, head_dim)\n",
    "        V = self.value(x) # (batch_size, seq_len, head_dim)\n",
    "\n",
    "        # Step 2: Compute scaled attention scores\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * self.head_dim**-0.5  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        masked_scores = torch.masked_fill(attention_scores, self.cmask[:seq_len, :seq_len]==0, float('-inf'))\n",
    "        attention_weights = F.softmax(masked_scores, dim=-1)\n",
    "        outputs = attention_weights @ V\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "attention = SingleHeadCausalAttention(config)\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, seq_len, d_model)\n",
    "output = attention(x)\n",
    "assert output.shape == (2, 10, 32)  # head_dim = 256/8 = 32\n",
    "print(\"Single-head causal attention output shape is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6kYTZAex6-I"
   },
   "source": [
    "**Reflect & Discuss**\n",
    "\n",
    "*   What does the causal mask accomplish? What would happen if we removed it?\n",
    "\n",
    "*   How do the key, query, and value projections differ conceptually?\n",
    "*  What do the attention weights represent?   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9026RCUAxmm8"
   },
   "source": [
    "Hint **Reflect & Discuss**\n",
    "\n",
    "*   What does the causal mask accomplish? What would happen if we removed it?\n",
    "\n",
    "*The causal mask ensures that each token can only attend to itself and to earlier tokens in the sequence. This enforces the left-to-right flow required for autoregressive language modeling, where the model predicts the next token based only on past context. If the mask were removed, tokens could attend to future positions, effectively letting the model “see the answer” during training and making it unusable for text generation, where future tokens aren’t yet known.*\n",
    "\n",
    "*   How do the key, query, and value projections differ conceptually?\n",
    "\n",
    "*The query represents what the current token is trying to find out — the kind of information it’s seeking from the rest of the sequence. The keys represent what information each token has to offer, and the values carry that actual information to be shared. The attention mechanism compares queries to keys to determine which values are most relevant, combining them into a context-aware representation for each position.*\n",
    "\n",
    "*  What do the attention weights represent?   \n",
    "\n",
    "*The attention weights represent how much importance the model assigns to each token when computing a new representation for the current position. After applying the softmax, they form a probability distribution over all tokens in the sequence, where higher weights indicate tokens the model finds more relevant or informative. In effect, the attention weights show where the model is “looking” in the context — which past words it considers most useful for understanding or predicting the current one.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HND-e28yKCma"
   },
   "source": [
    "# Multi-head self attention\n",
    "\n",
    "In transformers, multi-head attention allows the model to attend to information from different representation subspaces simultaneously. Each head runs self-attention independently, and the outputs are concatenated and linearly projected back into the model’s hidden dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz6qcH2aKCma"
   },
   "source": [
    "## Exercise: implementing multi-head attention\n",
    "\n",
    "\n",
    "In this exercise, you’ll implement the MultiHeadCausalAttention module using your SingleHeadCausalAttention from the previous exercise. You should not need to write more than a few lines of code here.\n",
    "\n",
    "1. Define the attention heads. Use `nn.ModuleList(...)` to create a list of attention heads (instances of `SingleHeadCausalAttention`) that will act in parallel on the input.  \n",
    "2. Define the output projection, using a linear layer, `self.linear`.\n",
    "3. Implement the forward pass.  The input `x` (which is shape (batch_size, seq_len, d_model)) should be passed through each head. You then concatenate the outputs of each head (you can use `torch.cat(...)`) and pass it through the linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lll7uEfibsRz"
   },
   "source": [
    "```python\n",
    "class MultiHeadCausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Create multiple attention heads\n",
    "        self.heads = nn.ModuleList([...])\n",
    "\n",
    "        # TODO: Final linear projection (d_model → d_model)\n",
    "        self.linear = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Pass input through all heads and concatenate the outputs\n",
    "        # Hint: use torch.cat(...) to combine head outputs\n",
    "        # Then apply the final linear layer\n",
    "        ...\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
    "out = mha(x)\n",
    "\n",
    "assert out.shape == (2, 10, 256)\n",
    "print(\"Multi-head causal attention output shape is correct!\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zO_MXe5KCma"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class MultiHeadCausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadCausalAttention(config) for _ in range(config.n_heads)])\n",
    "\n",
    "        self.linear = nn.Linear(config.d_model, config.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        y = self.linear(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(d_model=256, n_heads=8, ctx_len=16)\n",
    "mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "# Test with small batch\n",
    "x = torch.randn(2, 10, 256)  # (batch_size=2, seq_len=10, d_model=256)\n",
    "out = mha(x)\n",
    "assert out.shape == (2, 10, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JohlwotCKCma"
   },
   "source": [
    "# Define the full decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5esddY3EKCma"
   },
   "source": [
    "\n",
    "Below, we’ll first define the feedforward network. This module applies two linear transformations with a ReLU activation in between.\n",
    "The first layer expands the dimensionality by a factor of 4, and the second projects it back to the model dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkvHApztKCma"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(config.d_model, 4*config.d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4*config.d_model, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTjuI0uvKCma"
   },
   "source": [
    "## Exercise: Decoder Block\n",
    "\n",
    "Now you’ll implement a single decoder block, which is a core component that combines multi-head self-attention, the feed-forward network, and layer normalization.\n",
    "\n",
    "Each sublayer (MultiHeadAttention and the feedforward network) uses a residual connection following the pattern:\n",
    "\n",
    "x = x + sublayer(layer_norm(x))\n",
    "\n",
    "This order (LayerNorm → sublayer → residual) is used in GPT-style decoders because it improves stability during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz1uCoKCbz3O"
   },
   "source": [
    "```python\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadCausalAttention(config)\n",
    "\n",
    "        # TODO: Initialize layer normalization layers\n",
    "        # Hint: use nn.LayerNorm\n",
    "        self.ln1 = ...\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: residual around attention\n",
    "        x = ...\n",
    "\n",
    "        # TODO: residual around FFN\n",
    "        x = ...\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256)\n",
    "ffn = FFN(config)\n",
    "decoder = DecoderBlock(config)\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
    "output = decoder(x)\n",
    "\n",
    "assert output.shape == x.shape\n",
    "print(\"Decoder block output shape is correct!\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "191oZxvdKCmb",
    "outputId": "8b2c4024-5112-45cb-88c4-a22d247ba7a4"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadCausalAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "config = Config(d_model=256)\n",
    "decoder = DecoderBlock(config)\n",
    "\n",
    "# Test with random input\n",
    "x = torch.randn(2, 10, 256)  # (batch_size, sequence_length, d_model)\n",
    "output = decoder(x)\n",
    "assert output.shape == x.shape\n",
    "print(\"Decoder block output shape is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4O-4vGOKCmb"
   },
   "source": [
    "# Define the transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkfMrFuhKCmb"
   },
   "source": [
    "We're now ready to put the components together into our final decoder module that can actually generate text! This is the top-level module that:\n",
    "\n",
    "* Embeds input tokens and adds positional information\n",
    "* Processes them through multiple transformer layers\n",
    "* Outputs predictions for the next token through the `forward(...)` function\n",
    "* Can generate new sequences autoregressively through the `generate(...)` function\n",
    "\n",
    "Your task to implement the token generation method of the Decoder class below. In training, the model sees the entire sequence and learns to predict each token from the previous ones. At inference time, however, we don’t have the next token — we must generate it one at a time, feeding each newly generated token back into the model. This iterative process is called autoregressive generation. At every step, the model looks at the most recent context (up to ctx_len tokens), predicts a probability distribution over the vocabulary, samples one token, appends it, and repeats until it has produced max_len new tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXEi38cbb9vA"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of decoder blocks\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
    "\n",
    "        # Final layer norm (normalize across d_model dimension)\n",
    "        self.ln = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        # Linear projection from d_model to vocab_size\n",
    "        self.lin = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "        # Embeddings\n",
    "        self.emb = EmbeddingLayer(config)\n",
    "\n",
    "        # Loss function for training\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.ctx_len = config.ctx_len\n",
    "\n",
    "        self.device = config.device # don't change this (for training model on right device)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tokens (B, T)\n",
    "            targets: Optional target tokens (B, T)\n",
    "        Returns:\n",
    "            logits: Predictions (B, T, vocab_size)\n",
    "            loss: Optional cross-entropy loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Embed tokens (token + positional embeddings)\n",
    "        x = self.emb(x)\n",
    "\n",
    "        # Process through the stack of transformer blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Project from hidden dimension to vocabulary size\n",
    "        logits = self.lin(x)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size*seq_len, vocab_size)\n",
    "            targets = targets.view(batch_size*seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.L(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, token_ids, max_len=256):\n",
    "        \"\"\"\n",
    "        Generate new tokens given initial sequence of token IDs.\n",
    "\n",
    "        Args:\n",
    "            token_ids (torch.Tensor):\n",
    "                The starting sequence of token IDs, shape (batch_size, seq_len).\n",
    "            max_len (int, optional):\n",
    "                Maximum number of new tokens to generate.\n",
    "                Defaults to 256.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:\n",
    "                The complete sequence of generated token IDs, including both the\n",
    "                original input and the newly generated tokens.\n",
    "                Shape: (batch_size, seq_len + max_len).\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # TODO: Grab the last ctx_len tokens\n",
    "            token_window = ...\n",
    "\n",
    "            # TODO: Get model predictions\n",
    "            logits, _ = ...\n",
    "\n",
    "            # Only keep predictions for the last token\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # TODO: Sample the next token (hint, can use torch.multinomial)\n",
    "            next_token = ...\n",
    "\n",
    "            # TODO: Append next token to the sequence\n",
    "            token_ids = ...\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(\n",
    "    vocab_size=100,\n",
    "    d_model=256,\n",
    "    ctx_len=64,\n",
    "    n_layers=4\n",
    ")\n",
    "decoder = Decoder(config)\n",
    "\n",
    "x = torch.randint(0, 100, (1, 10))\n",
    "logits, loss = decoder(x, x)\n",
    "\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "assert out.shape == (1, 8)\n",
    "print(\"Decoder generation output shape is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-8OaUS6pYbr"
   },
   "source": [
    "Let’s inspect the architecture of our decoder to make sure it matches what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "id": "KAqoN0GzpX93",
    "outputId": "28db9cab-ef87-483e-8fda-f38e1164740e"
   },
   "source": [
    "```python\n",
    "print(decoder)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va2vJBjRKCme",
    "outputId": "ccd4f776-f363-493f-d0ea-d9df0d5aebfe"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of decoder blocks\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layers)])\n",
    "\n",
    "        # Final layer norm (normalize across d_model dimension)\n",
    "        self.ln = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        # Linear projection from d_model to vocab_size\n",
    "        self.lin = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "        # Embeddings\n",
    "        self.emb = EmbeddingLayer(config)\n",
    "\n",
    "        # Loss function for training\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.ctx_len = config.ctx_len\n",
    "\n",
    "        self.device = config.device # don't change this (for training model on right device)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tokens (B, T)\n",
    "            targets: Optional target tokens (B, T)\n",
    "        Returns:\n",
    "            logits: Predictions (B, T, vocab_size)\n",
    "            loss: Optional cross-entropy loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Embed tokens (token + positional embeddings)\n",
    "        x = self.emb(x)\n",
    "\n",
    "        # Process through the stack of transformer blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Project from hidden dimension to vocabulary size\n",
    "        logits = self.lin(x)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets for loss computation\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size*seq_len, vocab_size)\n",
    "            targets = targets.view(batch_size*seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.L(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, token_ids, max_len=256):\n",
    "        \"\"\"\n",
    "        Generate new tokens given initial sequence of token IDs.\n",
    "\n",
    "        Args:\n",
    "            token_ids (torch.Tensor):\n",
    "                The starting sequence of token IDs, shape (batch_size, seq_len).\n",
    "            max_len (int, optional):\n",
    "                Maximum number of new tokens to generate.\n",
    "                Defaults to 256.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:\n",
    "                The complete sequence of generated token IDs, including both the\n",
    "                original input and the newly generated tokens.\n",
    "                Shape: (batch_size, seq_len + max_len).\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # Grab the last ctx_len tokens\n",
    "            token_window = token_ids[:, -self.ctx_len:]\n",
    "\n",
    "            # Get model predictions\n",
    "            logits, _ = self(token_window)\n",
    "\n",
    "            # Only keep predictions for the last token\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token (hint, can use torch.multinomial)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append next token to the sequence\n",
    "            token_ids = torch.cat((token_ids, next_token), dim=1)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# Testing your implementation\n",
    "config = Config(\n",
    "    vocab_size=100,\n",
    "    d_model=256,\n",
    "    ctx_len=64,\n",
    "    n_layers=4\n",
    ")\n",
    "decoder = Decoder(config)\n",
    "\n",
    "x = torch.randint(0, 100, (1, 10))\n",
    "logits, loss = decoder(x, x)\n",
    "\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "out = decoder.generate(torch.tensor([[1, 2, 3]]), max_len=5)\n",
    "assert out.shape == (1, 8)\n",
    "print(\"Decoder output shape is correct!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPf-OThncDOG"
   },
   "source": [
    "**Reflect & Discuss**\n",
    "\n",
    "1.   What does `ctx_len` control, and what might happen if it’s too short or too long?\n",
    "2. How does the training objective relate to generation quality? If the model’s loss during training is low, does that always mean generation will sound good?\n",
    "3. During generation, what would happen if we always picked the most likely token instead of sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRY_0IzqnSTs"
   },
   "source": [
    "Hint **Reflect & Discuss**\n",
    "\n",
    "1.   What does `ctx_len` control, and what might happen if it’s too short or too long?\n",
    "\n",
    "*ctx_len determines how many of the most recent tokens the model can “see” when predicting the next one—it’s the size of the model’s working memory. If it’s too short, the model quickly forgets earlier parts of the sequence and can lose coherence, producing text that drifts off topic or repeats. If it’s very long, the model retains more context but becomes slower and more memory-hungry, since attention scales quadratically with sequence length. In practice, ctx_len is a trade-off between contextual understanding and computational efficiency.*\n",
    "\n",
    "2. How does the training objective relate to generation quality? If the model’s loss during training is low, does that always mean generation will sound good?\n",
    "\n",
    "*The training objective teaches the model to predict the next token given its context, minimizing cross-entropy loss. A low loss means the model is good at local next-token prediction on the training data. But fluent generation depends on repeatedly applying those predictions in sequence. Small local mistakes can compound over many steps, leading to incoherence, repetition, or drift. So low training loss is necessary but not sufficient for natural, high-quality text—generation quality also depends on how well the model generalizes and how sampling is performed during inference.*\n",
    "\n",
    "3. During generation, what would happen if we always picked the most likely token instead of sampling?\n",
    "\n",
    "*Always choosing the highest-probability token (taking the argmax) makes the process deterministic. The model would produce the same output every time for a given prompt, but the text often becomes repetitive or formulaic, because small biases toward common words get reinforced at each step. Sampling from the probability distribution adds randomness and variety, allowing the model to explore alternative word choices and generate more natural, creative language, even though it introduces some unpredictability.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHgj_4zOKCme"
   },
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBkUTqqX4uUk"
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "# Clear cached variables and free up GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxcymewqqD-k"
   },
   "source": [
    "Now that our model architecture is complete, we can train it to predict and generate text.\n",
    "Below, we define our model configuration, set up the optimizer, and run a simple training loop.\n",
    "Every few hundred steps, we’ll evaluate performance on the validation set and print a short sample of generated text to see how learning is progressing.\n",
    "By the end of training, we’ll use the fully trained model to generate a longer passage from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YX7BzDluKCme",
    "outputId": "444d5082-1cbd-49a1-a9fd-47cf34b47f77"
   },
   "outputs": [],
   "source": [
    "# Set up model\n",
    "config = Config(d_model=252, n_heads=12, ctx_len=128, batch_size = 64, n_layers = 12, device='cuda')\n",
    "config.set_vocab_size(vocab_size=len(chars))\n",
    "model = Decoder(config).to(config.device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {n_params}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 3e-4\n",
    "max_iters = 6000\n",
    "eval_interval = 200  # How often to evaluate\n",
    "eval_iters = 100     # How many batches to use for evaluation\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "    xb, yb = get_batch('train', config.ctx_len, config.batch_size, config.device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate periodically\n",
    "    if iter % eval_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for _ in range(eval_iters):\n",
    "                xb, yb = get_batch('val', config.ctx_len, config.batch_size, config.device)\n",
    "                _, val_loss = model(xb, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "            print(f\"step {iter}: train loss {loss.item():.4f}, val loss {avg_val_loss:.4f}\")\n",
    "\n",
    "            # Generate text sample to monitor learning progress\n",
    "            context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "            print(decode(model.generate(context, max_len=100)[0].tolist(), int_to_str))\n",
    "            print('='*50)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "# Final text generation\n",
    "model.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=config.device)\n",
    "print(\"\\nFinal generated text:\")\n",
    "print(decode(model.generate(context, max_len=500)[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KR4O9NFKCme"
   },
   "outputs": [],
   "source": [
    "# Save our model\n",
    "import os\n",
    "if not os.path.exists('model.pth'):\n",
    "   torch.save(model.state_dict(), 'model.pth')\n",
    "else:\n",
    "   print('Model file (model.pth) already exists!  Saving under a different name model_other.pth.')\n",
    "   torch.save(model.state_dict(), 'model_other.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaH1KrFLtWjN"
   },
   "source": [
    "# Evaluate the trained transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7dB0yrQtgfj"
   },
   "source": [
    "We provide a trained model with slightly larger dimensions and context size, which you are welcome to use! You can also comment out the below cell and use the model you trained above for this section if you were able to train it fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYWqt-fTtrpT"
   },
   "outputs": [],
   "source": [
    "# to load a saved model, uncomment the below code\n",
    "\n",
    "# url = \"https://osf.io/dt6h4/download\"   # replace with your actual OSF file link\n",
    "# r = requests.get(url)\n",
    "# with open(\"model_transformer.pth\", \"wb\") as f:\n",
    "#     f.write(r.content)\n",
    "\n",
    "# config = Config(d_model=768, n_heads=12, ctx_len=512, batch_size = 64, n_layers = 12, device='cuda:0')\n",
    "# config.set_vocab_size(vocab_size=len(chars))\n",
    "# model = Decoder(config).to(config.device)\n",
    "# model.load_state_dict(torch.load(\"transformer_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbrhl2-ouFoc"
   },
   "source": [
    "Let's see what your model generates! Try changing the **prompt** and **max_len** values below and observe how the text changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYcASRNtKCme"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt_ids = torch.tensor([encode(\"Neuroscience is\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "max_len = 512\n",
    "print(decode(model.generate(prompt_ids, max_len=max_len)[0].tolist(), int_to_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULNu3u_gv19N"
   },
   "source": [
    "Here you can generate text without sampling (by just taking the argmax of the logits at every point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MpqWGcRv74r"
   },
   "outputs": [],
   "source": [
    "# Deterministic (argmax) generation\n",
    "def generate_argmax(model, idx, max_len=200):\n",
    "    for _ in range(max_len):\n",
    "        idx_window = idx[:, -model.ctx_len:]\n",
    "        logits, _ = model(idx_window)\n",
    "        logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx\n",
    "\n",
    "prompt = \"Neuroscience is\"\n",
    "prompt_ids = torch.tensor([[str_to_int[c] for c in prompt]], device=config.device)\n",
    "sample = generate_argmax(model, prompt_ids.clone(), max_len=200)\n",
    "print(decode(sample[0].tolist(), int_to_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raMSJPf1ubjz"
   },
   "source": [
    "**Reflect and Discuss**\n",
    "\n",
    "* How does the model’s behavior differ when using sampling versus argmax?\n",
    "\n",
    "* With smapling, how coherent or structured does the text sound? Does the model stay on topic, or drift?\n",
    "\n",
    "* What happens if you start with a completely different prompt or punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ucNSatvA0w"
   },
   "source": [
    "## Extension: Add Temperature to Generation\n",
    "\n",
    "You might have noticed that your model sometimes repeats itself or produces very predictable text.  \n",
    "Let’s make generation more flexible by adding a **temperature** parameter that controls how random or “creative” the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idfS8_iVvJtG"
   },
   "outputs": [],
   "source": [
    "# Extended version of generate() with temperature control\n",
    "def generate_with_temperature(self, idx, max_len=256, temperature=1.0):\n",
    "    for _ in range(max_len):\n",
    "        idx_window = idx[:, -self.ctx_len:]\n",
    "        logits, _ = self(idx_window)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx\n",
    "\n",
    "# Attach the new method to the existing model\n",
    "from types import MethodType\n",
    "model.generate_with_temperature = MethodType(generate_with_temperature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGUAvp3vvPUm"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt_ids = torch.tensor([encode(\"Neuroscience is\", str_to_int)], dtype=torch.long, device=config.device)\n",
    "max_len = 512\n",
    "for temp in [0.3, 0.7, 1.0, 1.5, 2.0]:\n",
    "    print(f\"\\nTemperature = {temp}\")\n",
    "    sample = model.generate_with_temperature(\n",
    "        prompt_ids.clone(), max_len=200, temperature=temp\n",
    "    )\n",
    "    print(decode(sample[0].tolist(), int_to_str))\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQv1vdqDwfm8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
